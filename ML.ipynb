{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "from spacy.attrs import ORTH, LEMMA, NORM, TAG\n",
    "from clean import replace_emoticons, clean\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import ComplementNB, MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from joblib import dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 8836: expected 4 fields, saw 5\\n'\n",
      "b'Skipping line 535882: expected 4 fields, saw 7\\n'\n"
     ]
    }
   ],
   "source": [
    "twitter_full_df = pd.read_csv(\"Resources/sent_analysis_dataset.csv\", error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df = twitter_full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select slice to test\n",
    "twitter_df = twitter_full_df.loc[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df = twitter_full_df.sample(frac = .0025)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop and rename columns\n",
    "twitter_df = twitter_df.drop([\"ItemID\", \"SentimentSource\"], axis = 1)\n",
    "twitter_df = twitter_df.rename(columns = {\"SentimentText\": \"Text\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment    0\n",
       "Text         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check columns for missing data\n",
    "twitter_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment     int64\n",
       "Text         object\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify data are of correct type\n",
    "twitter_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify sentiment column has appropriate data\n",
    "twitter_df[\"Sentiment\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text of tweets using previously defined clean_tweet function\n",
    "twitter_df[\"Text\"] = twitter_df[\"Text\"].map(lambda x: clean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df.head()\n",
    "# Backup cleaned csv\n",
    "twitter_df.to_csv(\"Resources/clean_dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import spacy nlp library\n",
    "nlp = spacy.load('en_core_web_sm', entity=False)\n",
    "\n",
    "# Add customized stop words\n",
    "nlp.Defaults.stop_words |= {\"-PRON-\",\"joe\", \"biden\", \"bernie\",\"sanders\", \"elizabeth\", \\\n",
    "                            \"warren\", \"kamala\", \"harris\", \"s\", \"ve\", \"twitter\", \"tweet\",\\\n",
    "                            \"come\", \"year\", \"know\"}\n",
    "\n",
    "# Creating tokenizer function\n",
    "def spacy_tokenizer(tweet):\n",
    "    \n",
    "    # Parse tweets into tokens\n",
    "    tokens = nlp(tweet)\n",
    "\n",
    "    # Lemmatize tokens\n",
    "    tokens = [word.lemma_ for word in tokens if word.lemma_ not in nlp.Defaults.stop_words]\n",
    "\n",
    "    # Return list of tokens\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into testing and training datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into testing and training datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(twitter_df[\"Text\"], \\\n",
    "                                                    twitter_df[\"Sentiment\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test different models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create transformation pipeline\n",
    "pipeline = Pipeline([\n",
    "    (\"vectorize\", TfidfVectorizer(tokenizer = spacy_tokenizer, max_features = 1000)),\n",
    "    (\"classify\", MultinomialNB()),\n",
    "])\n",
    "\n",
    "# Set parameters for Gridsearch\n",
    "parameters = {\"vectorize__use_idf\": (True, False),\n",
    "              \"vectorize__ngram_range\": [(1,1), (1,2)],\n",
    "              \"vectorize__max_df\" : (1, .8),\n",
    "              \"vectorize__norm\": (\"l1\", \"l2\"),\n",
    "              \"classify__alpha\": (.8, 1)\n",
    "             }\n",
    "mnb = GridSearchCV(pipeline, param_grid = parameters, cv = 3, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 32 candidates, totalling 96 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  96 out of  96 | elapsed: 42.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('vectorize',\n",
       "                                        TfidfVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.float64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=1000,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        norm='l2',\n",
       "                                                        preprocessor=None,\n",
       "                                                        smooth_idf=True,\n",
       "                                                        stop_...\n",
       "                                        MultinomialNB(alpha=1.0,\n",
       "                                                      class_prior=None,\n",
       "                                                      fit_prior=True))],\n",
       "                                verbose=False),\n",
       "             iid='warn', n_jobs=None,\n",
       "             param_grid={'classify__alpha': (0.8, 1),\n",
       "                         'vectorize__max_df': (1, 0.8),\n",
       "                         'vectorize__ngram_range': [(1, 1), (1, 2)],\n",
       "                         'vectorize__norm': ('l1', 'l2'),\n",
       "                         'vectorize__use_idf': (True, False)},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=1)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.6773648648648649 with parameters: {'classify__alpha': 0.8, 'vectorize__max_df': 0.8, 'vectorize__ngram_range': (1, 1), 'vectorize__norm': 'l1', 'vectorize__use_idf': False}\n"
     ]
    }
   ],
   "source": [
    "# Get best results\n",
    "print(f\"Best Score: {mnb.best_score_} with parameters: {mnb.best_params_}\")\n",
    "optimized_mnb = mnb.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.6818642350557245\n",
      "Confusion matrix: [[374 123]\n",
      " [191 299]]\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.75      0.70       497\n",
      "           1       0.71      0.61      0.66       490\n",
      "\n",
      "    accuracy                           0.68       987\n",
      "   macro avg       0.69      0.68      0.68       987\n",
      "weighted avg       0.69      0.68      0.68       987\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model using test data\n",
    "predictions = optimized_mnb.predict(X_test)\n",
    "print(f\"Accuracy score: {accuracy_score(y_test, predictions)}\")\n",
    "print(f\"Confusion matrix: {confusion_matrix(y_test, predictions)}\")\n",
    "print(f\"Classification report: \\n{classification_report(y_test, predictions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complement Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create transformation pipeline\n",
    "pipeline = Pipeline([\n",
    "    (\"vectorize\", TfidfVectorizer(tokenizer = spacy_tokenizer)),\n",
    "    (\"classify\", ComplementNB()),\n",
    "])\n",
    "\n",
    "# Set parameters for Gridsearch\n",
    "parameters = {\"vectorize__use_idf\": (True, False),\n",
    "              \"vectorize__ngram_range\": [(1,1), (1,2)],\n",
    "              \"vectorize__max_df\" : (1, .8),\n",
    "              \"vectorize__norm\": (\"l1\", \"l2\"),\n",
    "              \"classify__alpha\": (.8, 1)\n",
    "             }\n",
    "cnb = GridSearchCV(pipeline, param_grid = parameters, cv = 3, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 32 candidates, totalling 96 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  96 out of  96 | elapsed: 168.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('vectorize',\n",
       "                                        TfidfVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.float64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        norm='l2',\n",
       "                                                        preprocessor=None,\n",
       "                                                        smooth_idf=True,\n",
       "                                                        stop_...\n",
       "                                        ComplementNB(alpha=1.0,\n",
       "                                                     class_prior=None,\n",
       "                                                     fit_prior=True,\n",
       "                                                     norm=False))],\n",
       "                                verbose=False),\n",
       "             iid='warn', n_jobs=None,\n",
       "             param_grid={'classify__alpha': (0.8, 1),\n",
       "                         'vectorize__max_df': (1, 0.8),\n",
       "                         'vectorize__ngram_range': [(1, 1), (1, 2)],\n",
       "                         'vectorize__norm': ('l1', 'l2'),\n",
       "                         'vectorize__use_idf': (True, False)},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=1)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnb.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.6905405405405406 with parameters: {'classify__alpha': 1, 'vectorize__max_df': 0.8, 'vectorize__ngram_range': (1, 1), 'vectorize__norm': 'l2', 'vectorize__use_idf': False}\n"
     ]
    }
   ],
   "source": [
    "# Get best results\n",
    "print(f\"Best Score: {cnb.best_score_} with parameters: {cnb.best_params_}\")\n",
    "optimized_cnb = cnb.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.6838905775075987\n",
      "Confusion matrix: [[373 124]\n",
      " [188 302]]\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.75      0.71       497\n",
      "           1       0.71      0.62      0.66       490\n",
      "\n",
      "    accuracy                           0.68       987\n",
      "   macro avg       0.69      0.68      0.68       987\n",
      "weighted avg       0.69      0.68      0.68       987\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model using test data\n",
    "predictions = optimized_cnb.predict(X_test)\n",
    "print(f\"Accuracy score: {accuracy_score(y_test, predictions)}\")\n",
    "print(f\"Confusion matrix: {confusion_matrix(y_test, predictions)}\")\n",
    "print(f\"Classification report: \\n{classification_report(y_test, predictions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['twitter_model.joblib']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save best model\n",
    "dump(optimized_mnb, 'twitter_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
