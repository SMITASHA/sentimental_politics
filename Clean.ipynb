{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "import json\n",
    "from spacy.attrs import ORTH, LEMMA, NORM, TAG\n",
    "# Load spacy model\n",
    "nlp = spacy.load('en_core_web_sm',parser=False, entity=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'whence', 'yours', 'than', '’ll', 'anywhere', 'afterwards', '‘re', 'nevertheless', 'many', 'due', '‘m', 'ourselves', 'she', 'less', 'noone', 'throughout', 'still', 'their', \"'d\", 'also', 'thereafter', 'ten', 'so', 'could', 'everything', 'very', 'however', '’re', 'nobody', 'cannot', 'its', 'was', 'towards', 'hereupon', \"'ve\", 'name', '.', 'already', 'except', 'whoever', 'hers', '’ve', 'put', 'anything', 'else', 'whereby', 'therein', 'where', 'are', 'has', 'more', 'therefore', 'via', 'us', 'rather', 'take', \"'ll\", 'in', 'somewhere', 'themselves', 'yourselves', 'then', 'forty', 'go', 'becoming', 'unless', 'twelve', 'as', 'among', 'per', 'yourself', 'ca', 'into', 'our', 'moreover', 'who', 'onto', 'nowhere', 'always', 'you', 'beforehand', 'please', 'last', 'amongst', 'herself', 'namely', 'move', 'through', 'same', 'another', 'were', 'beside', '’s', 'thru', 'thereupon', 'sometime', 'amount', 'besides', 'anyway', 'from', 'within', 'otherwise', 'at', '‘s', 'seem', 'although', 'whereafter', 'anyhow', 'next', 're', 'ever', 'upon', 'yet', 'will', 'her', 'fifty', 'ours', 'hence', 'mine', 'during', 'others', 'several', 'only', '‘ve', 'them', 'really', 'is', 'these', 'say', 'we', 'here', 'whenever', 'thence', 'why', 'those', 'own', 'two', 'latterly', 'elsewhere', 'thus', 'i', 'hereafter', 'they', 'meanwhile', 'until', 'which', 'my', 'me', 'all', 'been', 'together', 'for', 'around', \"'re\", 'quite', 'something', 'seeming', 'seemed', 'both', 'three', 'eight', 'before', 'may', 'this', 'too', 'using', 'used', 'about', 'and', 'much', 'because', 'herein', 'other', 'call', 'his', '‘ll', 'that', \"'m\", '’d', 'toward', 'do', \"'s\", 'any', 'would', 'off', 'or', 'whether', 'there', 'under', 'whole', 'someone', '‘d', 'might', 'while', 'whom', 'even', 'myself', 'your', 'himself', 'along', 'n‘t', 'just', 'see', 'whose', 'whatever', 'am', 'further', 'though', 'he', 'enough', 'almost', 'seems', 'it', 'of', 'beyond', 'does', 'everywhere', 'behind', 'if', 'back', 'hundred', 'had', 'must', 'what', 'most', 'get', 'every', 'perhaps', 'anyone', 'often', 'a', 'him', 'full', 'n’t', 'eleven', 'indeed', 'between', 'none', 'once', 'either', 'some', 'side', 'twenty', 'everyone', 'fifteen', 'whither', 'hereby', 'mostly', 'give', 'after', 'an', 'nine', '’m', 'four', 'whereas', 'sixty', 'wherein', 'the', 'six', 'somehow', 'now', '-PRON-', 'each', 'various', 'regarding', 'when', 'few', 'whereupon', 'out'}\n"
     ]
    }
   ],
   "source": [
    "# Load spacy model\n",
    "nlp = spacy.load('en_core_web_sm',parser=False, entity=False) \n",
    "\n",
    "\n",
    "# New stop words list \n",
    "nlp.Defaults.stop_words -= {'but', 'again', 'five', 'front','keep', 'by', 'with', 'nothing', 'be', 'make', 'on', 'can','down', \n",
    "                            'against', 'above', 'become','nor','serious', 'have', 'should','not','third','latter', 'never', 'becomes', 'doing', 'did', 'sometimes', 'since', 'to', 'show', 'across', \"n't\", 'itself', 'such',\n",
    "                            'formerly', 'how', 'part', 'thereby', 'wherever', 'done', 'bottom', 'least', 'alone', 'below',\n",
    "                            'former', 'made','first', 'top','being', 'up', 'one', 'neither', 'without', 'became', 'empty',\"will\"\n",
    "                            'over', 'no', 'well'}\n",
    "\n",
    "nlp.Defaults.stop_words.add('-PRON-')\n",
    "\n",
    "nlp.Defaults.stop_words.add('.')\n",
    "\n",
    "# '’m', 'toward', 'else', 'few', 'each', 'five', 'seems', 'must', 'front', 'onto', '‘m', 'yet', 'through', 'might',\n",
    "#'often', 'nevertheless', 'others', 'keep', 'by', 'his', 'with', 'nothing', 'beforehand', 'yourselves', 'amongst', \n",
    "#'say', 'be', 'enough', 'eight', 'yours', 'hereafter', 'make', 'an', 'among', 'after', 'when', 'she', 'sometime',\n",
    "#'will', 'beyond', 'on', 'you', 'from', 'perhaps', 'sixty', 'whereafter', 'anyway', 'amount', 'too', 'was', \"'ve\",\n",
    "#'noone', 'much', 'can', \"'re\", 'has', 'down', 'which', 'or', 'whatever', 'against', 'above', 'namely', 'become',\n",
    "#'the', 'rather', 'these', 'could', 'unless', 'ca', 'there', 'between', 'are', 'nor', 'under', 'used', 'everywhere', 'what',\n",
    "#'at', 'everything', 'them', 'very', 'who', 'now', 'call', 'before', 'several', '’s', 'anyone', 'side', 'would',\n",
    "#'hereupon', 'something', 'am', 'move', 'eleven', 'serious', 'have', 'someone', 'us', 'should', 'thru', 'indeed',\n",
    "#'though', 'my', 'most', 'next', '’re', 'not', 'latterly', 'third', '’ve', '‘d', 'seemed', 'still', 'any', 'themselves',\n",
    "#'does', 'six', 'almost', 'already', 'because', 'fifteen', 'thereupon', 'nowhere', \"'d\", 'latter', 'anyhow', 'same', \n",
    "#'so', 'only', 'whom', 'also', 'another', 'moreover', 'me', 'whether', 'further', 'however', 'whose', 'been', 'nine', \n",
    "#'afterwards', 'and', 'himself', 'whence', 'we', 'somewhere', 'ourselves', '’d', 'always', 'never', 'whereupon', 'whither',\n",
    "#'both', 'whoever', 'meanwhile', 'becomes', 'whereas', 'mostly', 'those', 'via', 'anywhere', 'doing', 'he', 'thence', 'did',\n",
    "#'sometimes', 'since', 'to', 'show', 'across', \"n't\", 'itself', 'such', 'formerly', 'how', 'part', 'thereby', 'wherever',\n",
    "#'done', '’ll', 'as', 'bottom', 'least', 'alone', 'below', 'just', 'forty', 'while', 'into', 'our', 'therefore', 'i', \n",
    "#'except', 'during', 'becoming', 'more', 'due', 'using', 'get', 'please', 'its', 'thus', 'none', 'former', 'see', '‘ve',\n",
    "#'made', 'name', 'it', 'in', 'may', 'hence', 'mine', 'hereby', 'hers', 'every', 'why', 'her', 'back', 'first', 'herein', \n",
    "#\"'ll\", 'other', 'per', 'if', 'ours', 'all', '‘s', 'take', 'beside', \"'m\", 'n‘t', 'go', 'top', 'here', 'regarding', 'within',\n",
    "#'had', 'their', 'last', 'being', 'give', 'off', 'full', 'twelve', 'up', 'ten', 'own', 'whenever', 'that', 'is', 'one',\n",
    "#'neither', 'fifty', 'everyone', 'otherwise', 'seem', 'myself', 'along', 'until', 'yourself', 'of', 'three', 'although',\n",
    "#'him', 'either', 'put', 'seeming', 'somehow', 'together', 'wherein', 'they', 'besides', 'then', \"'s\", 'anything', \n",
    "#'thereafter', 'without', '‘re', 'some', 'became', 'even', 'less', 'towards', 'twenty', 'were', 'whole', 'upon', 'your',\n",
    "#'various', 'around', 'behind', 'do', 'once', 'out', 'two', 'throughout', 'quite', 'therein', 'hundred', 'empty', 'over',\n",
    "#'where', 'for', 'about', 'n’t', 'whereby', 'elsewhere', 'nobody', 're', 'really', 'ever', 'no', 'many', 'well', 'a', \n",
    "#'four', 'this', 'herself', 'cannot', 'than', '‘ll'\n",
    "\n",
    "stops = nlp.Defaults.stop_words\n",
    "print (stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oh no not can not not know be to\n"
     ]
    }
   ],
   "source": [
    "\n",
    "TOKENIZER_EXCEPTIONS = {\n",
    "# do\n",
    "    \"don't\": [\n",
    "        {ORTH: \"do\", LEMMA: \"do\"},\n",
    "        {ORTH: \"n't\", LEMMA: \"not\", NORM: \"not\", TAG: \"RB\"}],\n",
    "    \"doesn't\": [\n",
    "        {ORTH: \"does\", LEMMA: \"do\"},\n",
    "        {ORTH: \"n't\", LEMMA: \"not\", NORM: \"not\", TAG: \"RB\"}],\n",
    "    \"didn't\": [\n",
    "        {ORTH: \"did\", LEMMA: \"do\"},\n",
    "        {ORTH: \"n't\", LEMMA: \"not\", NORM: \"not\", TAG: \"RB\"}],\n",
    "# can\n",
    "    \"can't\": [\n",
    "        {ORTH: \"ca\", LEMMA: \"can\"},\n",
    "        {ORTH: \"n't\", LEMMA: \"not\", NORM: \"not\", TAG: \"RB\"}],\n",
    "    \"couldn't\": [\n",
    "        {ORTH: \"could\", LEMMA: \"can\"},\n",
    "        {ORTH: \"n't\", LEMMA: \"not\", NORM: \"not\", TAG: \"RB\"}],\n",
    "# have\n",
    "    \"I've'\": [\n",
    "        {ORTH: \"I\", LEMMA: \"I\"},\n",
    "        {ORTH: \"'ve'\", LEMMA: \"have\", NORM: \"have\", TAG: \"VERB\"}],\n",
    "    \"haven't\": [\n",
    "        {ORTH: \"have\", LEMMA: \"have\"},\n",
    "        {ORTH: \"n't\", LEMMA: \"not\", NORM: \"not\", TAG: \"RB\"}],\n",
    "    \"hasn't\": [\n",
    "        {ORTH: \"has\", LEMMA: \"have\"},\n",
    "        {ORTH: \"n't\", LEMMA: \"not\", NORM: \"not\", TAG: \"RB\"}],\n",
    "    \"hadn't\": [\n",
    "        {ORTH: \"had\", LEMMA: \"have\"},\n",
    "        {ORTH: \"n't\", LEMMA: \"not\", NORM: \"not\", TAG: \"RB\"}],\n",
    "# will/shall will be replaced by will\n",
    "    \"I'll'\": [\n",
    "        {ORTH: \"I\", LEMMA: \"I\"},\n",
    "        {ORTH: \"'ll'\", LEMMA: \"will\", NORM: \"will\", TAG: \"VERB\"}],\n",
    "    \"he'll'\": [\n",
    "        {ORTH: \"he\", LEMMA: \"he\"},\n",
    "        {ORTH: \"'ll'\", LEMMA: \"will\", NORM: \"will\", TAG: \"VERB\"}],\n",
    "    \"she'll'\": [\n",
    "        {ORTH: \"she\", LEMMA: \"she\"},\n",
    "        {ORTH: \"'ll'\", LEMMA: \"will\", NORM: \"will\", TAG: \"VERB\"}],\n",
    "    \"it'll'\": [\n",
    "        {ORTH: \"it\", LEMMA: \"it\"},\n",
    "        {ORTH: \"'ll'\", LEMMA: \"will\", NORM: \"will\", TAG: \"VERB\"}],\n",
    "    \"won't\": [\n",
    "        {ORTH: \"wo\", LEMMA: \"will\"},\n",
    "        {ORTH: \"n't\", LEMMA: \"not\", NORM: \"not\", TAG: \"RB\"}],\n",
    "    \"wouldn't\": [\n",
    "        {ORTH: \"would\", LEMMA: \"will\"},\n",
    "        {ORTH: \"n't\", LEMMA: \"not\", NORM: \"not\", TAG: \"RB\"}],\n",
    "# be\n",
    "    \"I'm'\": [\n",
    "        {ORTH: \"I\", LEMMA: \"I\"},\n",
    "        {ORTH: \"'m'\", LEMMA: \"be\", NORM: \"am\", TAG: \"VERB\"}]\n",
    "}\n",
    "for tok, rule in TOKENIZER_EXCEPTIONS.items():\n",
    "        nlp.tokenizer.add_special_case('u'+tok, rule)\n",
    "\n",
    "# \"\"\"\"\"\"\n",
    "# def clean(nlp, doc):\n",
    "#     doc = nlp(doc)\n",
    "#     words = []\n",
    "#     for token in doc:\n",
    "#         words.append(token.lemma_)\n",
    "#     return ' '.join(words)\n",
    "    \n",
    "\n",
    "doc =nlp(u\"Oh no he didn't. I can't and I won't. I'll know what I'm gonna do.\")\n",
    "temp=[]\n",
    "for token in doc:\n",
    "    word = token.lemma_\n",
    "    if word not in nlp.Defaults.stop_words:\n",
    "        temp.append(token.lemma_)\n",
    "    \n",
    "    \n",
    "temp = ' '.join(temp)\n",
    "print(temp)\n",
    "    \n",
    "\n",
    "# \"\"\"   \n",
    "\n",
    "# temp=[]\n",
    "# #testing all contractions using spaCy's update tokenizer\n",
    "# doc =nlp(u\"Oh no he didn't. I can't and I won't. I'll know what I'm gonna do.\")\n",
    "# nlp.tokenizer.add_special_case(doc,TOKENIZER_EXCEPTIONS)\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "# lemmas = self._lemmatizer.lemmatize(token.text, pos_universal_google=token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I dog Happy\n"
     ]
    }
   ],
   "source": [
    "# Replace happy and sad emoticons with words \"happy\" and \"sad\"\n",
    "# from: https://towardsdatascience.com/extracting-twitter-data-pre-processing-and-sentiment-analysis-using-python-3-0-7192bd8b47cf)\n",
    "\n",
    "emoticons ={'Happy': [\n",
    "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "    '<3'\n",
    "    ], \n",
    "            'Sad': [\n",
    "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "    ':c', ':{', '>:\\\\', ';('\n",
    "    ]}\n",
    "a=\"I dog ;)\"\n",
    "temp=\"\"\n",
    "# for word in a.split():\n",
    "#      print (emoticons.get(word, word))\n",
    "    \n",
    "converted = []\n",
    "for word in a.split():\n",
    "    new_word = word\n",
    "    for emotion_key, emotion_val in emoticons.items():\n",
    "        if word in emotion_val:\n",
    "            new_word = emotion_key\n",
    "            break\n",
    "    converted.append(new_word)\n",
    "        \n",
    "        \n",
    "converted = ' '.join(converted)\n",
    "        \n",
    "print(converted)\n",
    "    \n",
    "    \n",
    "# for word in emoticons.values():\n",
    "#     for search in a.split():\n",
    "#         if :\n",
    "#             temp1=emoticons[search]\n",
    "#         else:\n",
    "#             temp1=search\n",
    "#         temp=temp+\" \" + temp1\n",
    "            \n",
    "# print (temp)\n",
    "        \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sample @sentence, I didn't & make it.\n"
     ]
    }
   ],
   "source": [
    "# Load the language model and parse your document.\n",
    "nlp =spacy.load('en_core_web_sm')  # en for English; others available.\n",
    "tweet = \"This is a sample @sentence, I didn't & make it.\"\n",
    "# doc = nlp(text)\n",
    "# print (doc)\n",
    "# doc = abc sad &abc\n",
    "# def process_tweet(tweet):\n",
    "     # Remove HTML special entities (e.g. &amp;)\n",
    "tweet = re.sub(r'\\&\\w*;', '', tweet)\n",
    "print (tweet)\n",
    "#     #Convert @username to AT_USER\n",
    "#     tweet = re.sub('@[^\\s]+','',tweet)\n",
    "#     # Remove tickers\n",
    "#     tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "#     # To lowercase\n",
    "#     tweet = tweet.lower()\n",
    "#     # Remove hyperlinks\n",
    "#     tweet = re.sub(r'https?:\\/\\/.*\\/\\w*', '', tweet)\n",
    "\n",
    "#          tweet = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\), ]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',\"  \",tweet)\n",
    "#     # Remove hashtags\n",
    "#     tweet = re.sub(r'#\\w*', '', tweet)\n",
    "#     # Remove Punctuation and split 's, 't, 've with a space for filter\n",
    "#     tweet = re.sub(r'[' + punctuation.replace('@', '') + ']+', ' ', tweet)\n",
    "#     # Remove words with 2 or fewer letters\n",
    "#     tweet = re.sub(r'\\b\\w{1,2}\\b', '', tweet)\n",
    "#     # Remove whitespace (including new line characters)\n",
    "#     tweet = re.sub(r'\\s\\s+', ' ', tweet)\n",
    "#     # Remove single space remaining at the front of the tweet.\n",
    "#     tweet = tweet.lstrip(' ') \n",
    "#     # Remove characters beyond Basic Multilingual Plane (BMP) of Unicode:\n",
    "#     tweet = ''.join(c for c in tweet if c <= '\\uFFFF') \n",
    "#     return tweet\n",
    "\n",
    "\n",
    "# clean_tweet=process_tweet(doc)\n",
    "\n",
    "\n",
    "\n",
    "# print (clean_tweet)\n",
    "\n",
    "# # Perform your operations over the tokens of the document.\n",
    "# for token in doc:\n",
    "#      temp=process_token(token)\n",
    "# #     print (token)\n",
    "   \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "# # Alternatively, index or slice into doc as if it were a list.\n",
    "# process_token(doc[234])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df = pd.read_csv(\"data/BidenTrain.csv\", error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For right now, we will only use part of the set\n",
    "# twitter_df = twitter_df[:-1000000]\n",
    "# len(twitter_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Favorite</th>\n",
       "      <th>ID</th>\n",
       "      <th>Retweet</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-07-31 13:20:35</td>\n",
       "      <td>0</td>\n",
       "      <td>1156555255473160194</td>\n",
       "      <td>0</td>\n",
       "      <td>-@CNN doesn't like @BernieSanders pointing out...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-07-31 13:20:32</td>\n",
       "      <td>0</td>\n",
       "      <td>1156555245700468742</td>\n",
       "      <td>0</td>\n",
       "      <td>@JoeBiden Disingenuous - #MedicareForAll Joe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-07-31 13:20:27</td>\n",
       "      <td>0</td>\n",
       "      <td>1156555221054758912</td>\n",
       "      <td>0</td>\n",
       "      <td>@TomPerez Now tonight we need @CoryBooker @Sen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-07-31 13:20:25</td>\n",
       "      <td>0</td>\n",
       "      <td>1156555213710471168</td>\n",
       "      <td>0</td>\n",
       "      <td>Looking for a logo for National Twin Day. What...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-07-31 13:20:18</td>\n",
       "      <td>0</td>\n",
       "      <td>1156555183385694209</td>\n",
       "      <td>0</td>\n",
       "      <td>@CassLovesToTan @JoeBiden @joncoopertweets Tru...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Date  Favorite                   ID  Retweet  \\\n",
       "0  2019-07-31 13:20:35         0  1156555255473160194        0   \n",
       "1  2019-07-31 13:20:32         0  1156555245700468742        0   \n",
       "2  2019-07-31 13:20:27         0  1156555221054758912        0   \n",
       "3  2019-07-31 13:20:25         0  1156555213710471168        0   \n",
       "4  2019-07-31 13:20:18         0  1156555183385694209        0   \n",
       "\n",
       "                                                Text  \n",
       "0  -@CNN doesn't like @BernieSanders pointing out...  \n",
       "1       @JoeBiden Disingenuous - #MedicareForAll Joe  \n",
       "2  @TomPerez Now tonight we need @CoryBooker @Sen...  \n",
       "3  Looking for a logo for National Twin Day. What...  \n",
       "4  @CassLovesToTan @JoeBiden @joncoopertweets Tru...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date        object\n",
       "Favorite     int64\n",
       "ID           int64\n",
       "Retweet      int64\n",
       "Text        object\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify data are of correct type\n",
    "twitter_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Verify sentiment column has appropriate data\n",
    "# twitter_df[\"Sentiment\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# twitter_df[\"X\"] = twitter_df[\"SentimentText\"]\n",
    "# twitter_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"test string \"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smita call the function at the end\n",
    "# twitter_df[\"Text\"] = twitter_df[\"Text\"].map(lambda x: clean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decided not to replace text abbreviations with full words b/c the abbreviations carry their own meaning\n",
    "# Decided to keep hashtags other than the # b/c they may carry their own meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming? Lemma?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace \"n't\" with \" not\" - GRETEL FIGURE OUT HOW TO TO THIS\n",
    "twitter_df[\"X\"] = twitter_df[\"X\"].map(lambda x: re.sub(r\"n't\", \" \", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing everything with a space that I will remove later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove links and html special entities\n",
    "twitter_df[\"X\"] = twitter_df[\"X\"].map(lambda x: re.sub(\"http*\", \" \", x))\n",
    "twitter_df[\"X\"] = twitter_df[\"SentimentText\"].map(lambda x: re.sub(r'&\\w*;', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove handles\n",
    "twitter_df[\"X\"] = twitter_df[\"X\"].map(lambda x: re.sub(\"r^@\", \"\", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace happy and sad emoticons with words \"happy\" and \"sad\"\n",
    "# from: https://towardsdatascience.com/extracting-twitter-data-pre-processing-and-sentiment-analysis-using-python-3-0-7192bd8b47cf)\n",
    "emoticons_happy = set([\n",
    "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "    '<3'\n",
    "    ])\n",
    "\n",
    "emoticons_sad = set([\n",
    "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "    ':c', ':{', '>:\\\\', ';('\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRETEL - FIGURE OUT HOW TO DO THIS\n",
    "twitter_df[\"X\"] = twitter_df[\"SentimentText\"].map(lambda x: re.sub(emoticons_happy, \"happy\", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove special characters\n",
    "\n",
    "twitter_df[\"X\"] = twitter_df[\"SentimentText\"].map(lambda x: re.sub(r\"[!#$%&'\\\\()*+,-./:;<=>?@\\^_`{|}~]\", \"\", x))\n",
    "twitter_df[\"X\"] = twitter_df[\"X\"].map(lambda x: re.sub(\"\\[\", \" \", x))\n",
    "twitter_df[\"X\"] = twitter_df[\"X\"].map(lambda x: re.sub(\"\\]\", \" \", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123  45\n"
     ]
    }
   ],
   "source": [
    "x = \"123!#$%&'()*+,-./:;<=>?@[\\]^_`{|}~45\"\n",
    "y = re.sub(r\"[!#$%&'\\\\()*+,-./:;<=>?@\\^_`{|}~]\", \"\", x)\n",
    "           \n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All extra whitespace\n",
    "twitter_df[\"X\"] = twitter_df[\"X\"].map(lambda x: re.sub(\"\\s\", \" \", x))\n",
    "twitter_df[\"X\"] = twitter_df[\"X\"].map(lambda x: re.sub(\"  \", \"\", x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into testing and training datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation?\n",
    "What cross-validation does is splitting the training data into a certain number of training folds \n",
    "(with 75% of the training data) and a the same number of testing folds (with 25% of the training data), \n",
    "use the training folds to train the classifier, and test it against the testing folds to obtain performance \n",
    "metrics (see below). The process is repeated multiple times and an average for each of the metrics is calculated.\n",
    "\n",
    "If your testing set is always the same, you might be overfitting to that testing set, which means you might \n",
    "be adjusting your analysis to a given set of data so much that you might fail to analyze a different set. \n",
    "Cross-validation helps prevent that.\n",
    "The more data you have, the more folds you will be able to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply algorithm (1 to begin)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (PythonData)",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
