{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "import re\n",
    "import spacy\n",
    "from spacy.attrs import ORTH, LEMMA, NORM, TAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_emoticons(string):\n",
    "    \"\"\"Replace emoticons with positive or negative words\"\"\"\n",
    "\n",
    "    \n",
    "    # Define emoticons to be replaced\n",
    "    emoticons ={'Good': [':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)',\\\n",
    "                          ':}', ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D',\\\n",
    "                          '=D', '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P',\\\n",
    "                          ':P', 'X-P','x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)',\\\n",
    "                          '>;)', '>:-)', '<3'],\\\n",
    "                'Bad': [':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\\\n",
    "                        ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\\\n",
    "                        ':c', ':{', '>:\\\\', ';(']}\n",
    "    \n",
    "    # If a string in a tweet is an emoticon, replace that emoticon with positive/negative word\n",
    "    for emoticon_key, emoticon_val in emoticons.items():\n",
    "        if string in emoticon_val:\n",
    "            string = emoticon_key\n",
    "            break\n",
    "        \n",
    "    return(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(string):\n",
    "    \"\"\"Cleans given string from tweet to prepare for using in machine learning model\"\"\"\n",
    "    \n",
    "    \n",
    "    # Make lowercase\n",
    "    string = string.lower()\n",
    "    # Replace emoticons\n",
    "    string = replace_emoticons(string)\n",
    "    # Replace emojis\n",
    "    string = re.sub(r'[^\\x00-\\x7F]+','', string)\n",
    "    # Remove hyperlinks\n",
    "    string = re.sub(r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\), ]|(?:%[0-9a-fA-F][0-9a-fA-F]))\\S+\",\"\", string)\n",
    "    # Remove HTML special entities\n",
    "    string = re.sub(r\"\\&\\w*;\",\" \", string)\n",
    "    # Convert remove twitter usernames\n",
    "    string = re.sub(r\"@[^\\s]+\",\"\", string)\n",
    "    # Remove 1 letter words\n",
    "    string = re.sub(r\"\\W*\\b\\w\\b\", \"\", string)\n",
    "    # Remove numbers\n",
    "    string = re.sub(\"\\d+\", \"\", string)\n",
    "    # Remove sepcial characters\n",
    "    string = re.sub(r\"[!#$%&'()*+\\\",.:;<=>?@^_`{|}~-\\\\\\/\\]\\[]\", \" \", string)\n",
    "\n",
    "    return(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(tweet, nlp):\n",
    "    \"\"\"Lemmatizes tweet and replaces stop words\"\"\"\n",
    "    \n",
    "    \n",
    "    # Define contractions to be replaced\n",
    "    TOKENIZER_EXCEPTIONS = {\n",
    "        \"don't\": [\n",
    "            {ORTH: \"do\", LEMMA: \"do\"},\n",
    "            {ORTH: \"n't\", LEMMA: \"not\", NORM: \"not\", TAG: \"RB\"}],\n",
    "        \"doesn't\": [\n",
    "            {ORTH: \"does\", LEMMA: \"do\"},\n",
    "            {ORTH: \"n't\", LEMMA: \"not\", NORM: \"not\", TAG: \"RB\"}],\n",
    "        \"didn't\": [\n",
    "            {ORTH: \"did\", LEMMA: \"do\"},\n",
    "            {ORTH: \"n't\", LEMMA: \"not\", NORM: \"not\", TAG: \"RB\"}],\n",
    "        \"can't\": [\n",
    "            {ORTH: \"ca\", LEMMA: \"can\"},\n",
    "            {ORTH: \"n't\", LEMMA: \"not\", NORM: \"not\", TAG: \"RB\"}],\n",
    "        \"couldn't\": [\n",
    "            {ORTH: \"could\", LEMMA: \"can\"},\n",
    "            {ORTH: \"n't\", LEMMA: \"not\", NORM: \"not\", TAG: \"RB\"}],\n",
    "        \"haven't\": [\n",
    "            {ORTH: \"have\", LEMMA: \"have\"},\n",
    "            {ORTH: \"n't\", LEMMA: \"not\", NORM: \"not\", TAG: \"RB\"}],\n",
    "        \"hasn't\": [\n",
    "            {ORTH: \"has\", LEMMA: \"have\"},\n",
    "            {ORTH: \"n't\", LEMMA: \"not\", NORM: \"not\", TAG: \"RB\"}],\n",
    "        \"hadn't\": [\n",
    "            {ORTH: \"had\", LEMMA: \"have\"},\n",
    "            {ORTH: \"n't\", LEMMA: \"not\", NORM: \"not\", TAG: \"RB\"}],\n",
    "        \"won't\": [\n",
    "            {ORTH: \"wo\", LEMMA: \"will\"},\n",
    "            {ORTH: \"n't\", LEMMA: \"not\", NORM: \"not\", TAG: \"RB\"}],\n",
    "        \"wouldn't\": [\n",
    "            {ORTH: \"would\", LEMMA: \"will\"},\n",
    "            {ORTH: \"n't\", LEMMA: \"not\", NORM: \"not\", TAG: \"RB\"}]\n",
    "    }\n",
    "    \n",
    "    # Add contractions and replacements to tokenizer list\n",
    "    for tok, rule in TOKENIZER_EXCEPTIONS.items():\n",
    "        nlp.tokenizer.add_special_case('u'+tok, rule)\n",
    "\n",
    "    # Remove words from default stop word list that may have impact on sentiment\n",
    "    nlp.Defaults.stop_words -= {'but', 'again', 'front','keep', 'nothing', 'can', \"n't\"\\\n",
    "                                'down','against', 'above', 'nor', 'serious', 'should',\\\n",
    "                                'not', 'never', 'across', 'bottom', 'least', 'alone', \\\n",
    "                                'below','first', 'top', 'up', 'neither', 'without', \\\n",
    "                                'empty', 'over', 'no', 'well'}\n",
    "    \n",
    "    # Add customized stop words\n",
    "    nlp.Defaults.stop_words |= {{\"-PRON-\",\"joe\", \"biden\", \"bernie\",\"sanders\", \\\n",
    "                                 \"elizabeth\", \"warren\", \"kamala\", \"harris\"}}\n",
    "    \n",
    "    # Lemmatize tweet\n",
    "    doc = nlp(tweet)\n",
    "    \n",
    "    # Create empty list for cleaned text\n",
    "    text = []\n",
    "    \n",
    "    # Clean words in tweet\n",
    "    for token in doc:\n",
    "        string = token.lemma_\n",
    "        \n",
    "        # Verify not a stop word and clean\n",
    "        if string not in nlp.Defaults.stop_words:\n",
    "            string = clean_text(string)\n",
    "            \n",
    "            # Add only non-empty strings to text list\n",
    "            if string.strip():\n",
    "                # Verify cleaned word is not stop word:\n",
    "                if string not in nlp.Defaults.stop_words:\n",
    "                    text.append(string)\n",
    "\n",
    "    # If no text is left, return null; otherwise, return cleaned tweet as single string\n",
    "    if not text:\n",
    "        return(None)\n",
    "    else:\n",
    "        return(' '.join(text))\n",
    "    \n",
    "    return(tweet_cleaned)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
