{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "#from clean import clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 8836: expected 4 fields, saw 5\\n'\n",
      "b'Skipping line 535882: expected 4 fields, saw 7\\n'\n"
     ]
    }
   ],
   "source": [
    "twitter_df = pd.read_csv(\"Resources/sent_analysis_dataset.csv\", error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78612"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For right now, we will only use part of the set\n",
    "twitter_df = twitter_df[:-1500000]\n",
    "len(twitter_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentSource</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ItemID  Sentiment SentimentSource  \\\n",
       "0       1          0    Sentiment140   \n",
       "1       2          0    Sentiment140   \n",
       "2       3          1    Sentiment140   \n",
       "3       4          0    Sentiment140   \n",
       "4       5          0    Sentiment140   \n",
       "\n",
       "                                       SentimentText  \n",
       "0                       is so sad for my APL frie...  \n",
       "1                     I missed the New Moon trail...  \n",
       "2                            omg its already 7:30 :O  \n",
       "3            .. Omgaga. Im sooo  im gunna CRy. I'...  \n",
       "4           i think mi bf is cheating on me!!!   ...  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop and rename columns\n",
    "twitter_df = twitter_df.drop([\"ItemID\", \"SentimentSource\"], axis = 1)\n",
    "twitter_df = twitter_df.rename(columns = {\"SentimentText\": \"Text\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment    0\n",
       "Text         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check columns for missing data\n",
    "twitter_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment     int64\n",
       "Text         object\n",
       "dtype: object"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify data are of correct type\n",
    "twitter_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify sentiment column has appropriate data\n",
    "twitter_df[\"Sentiment\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean data using clean function\n",
    "\n",
    "# twitter_df[\"Sentiment\"] = twitter_df[\"Sentiment\"].map(lambda x: clean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET RID OF THIS ONCE WE HAVE CLEAN FUNCTION\n",
    "twitter_df[\"Text\"] = twitter_df[\"Text\"].map(lambda x: re.sub(r\"[!#$%&'\\\\()*+,-./:;<=>?@\\^_`{|}~]\", \"\", x))\n",
    "twitter_df[\"Text\"] = twitter_df[\"Text\"].map(lambda x: re.sub(\"\\[\", \" \", x))\n",
    "twitter_df[\"Text\"] = twitter_df[\"Text\"].map(lambda x: re.sub(\"\\]\", \" \", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>is so sad for my APL friend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>I missed the New Moon trailer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>omg its already 730 O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Omgaga Im sooo  im gunna CRy Ive be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>i think mi bf is cheating on me       TT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>or i just worry too much</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>Juuuuuuuuuuuuuuuuussssst Chillin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>Sunny Again        Work Tomorrow       ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>handed in my uniform today  i miss you a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>hmmmm i wonder how she my number</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>I must think about positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>thanks to all the haters up in my face a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>this weekend has sucked so far</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>jb isnt showing in australia any more</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>ok thats it you win</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>lt This is the way i feel right now</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>awhhe man Im completely useless rt now Fun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>Feeling strangely fine Now Im gonna go lis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>HUGE roll of thunder just nowSO scary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>I just cut my beard off Its only been grow...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentiment                                               Text\n",
       "0           0                        is so sad for my APL friend\n",
       "1           0                      I missed the New Moon trailer\n",
       "2           1                              omg its already 730 O\n",
       "3           0             Omgaga Im sooo  im gunna CRy Ive be...\n",
       "4           0           i think mi bf is cheating on me       TT\n",
       "5           0                   or i just worry too much        \n",
       "6           1                   Juuuuuuuuuuuuuuuuussssst Chillin\n",
       "7           0         Sunny Again        Work Tomorrow       ...\n",
       "8           1        handed in my uniform today  i miss you a...\n",
       "9           1                  hmmmm i wonder how she my number \n",
       "10          0                        I must think about positive\n",
       "11          1        thanks to all the haters up in my face a...\n",
       "12          0                     this weekend has sucked so far\n",
       "13          0              jb isnt showing in australia any more\n",
       "14          0                                ok thats it you win\n",
       "15          0                lt This is the way i feel right now\n",
       "16          0      awhhe man Im completely useless rt now Fun...\n",
       "17          1      Feeling strangely fine Now Im gonna go lis...\n",
       "18          0              HUGE roll of thunder just nowSO scary\n",
       "19          0      I just cut my beard off Its only been grow..."
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PARAMETERS IN TFIDVECTORIZER\n",
    "\n",
    "decode_error : {‘strict’, ‘ignore’, ‘replace’} (default=’strict’)\n",
    "Instruction on what to do if a byte sequence is given to analyze that contains characters not of the given encoding. By default, it is ‘strict’, meaning that a UnicodeDecodeError will be raised. Other values are ‘ignore’ and ‘replace’.\n",
    "\n",
    "IF TOO MANY FEATURES, ADJUST HERE\n",
    "max_df : float in range [0.0, 1.0] or int (default=1.0)\n",
    "When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words). If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "min_df : float in range [0.0, 1.0] or int (default=1)\n",
    "When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "DON'T CHANGE, BUT INCLUDE IN README THAT WE ARE DOING THE DEFAULTS AND WHY\n",
    "\n",
    "norm : ‘l1’, ‘l2’ or None, optional (default=’l2’)\n",
    "Each output row will have unit norm, either: * ‘l2’: Sum of squares of vector elements is 1. The cosine similarity between two vectors is their dot product when l2 norm has been applied. * ‘l1’: Sum of absolute values of vector elements is 1. See preprocessing.normalize\n",
    "\n",
    "use_idf : boolean (default=True)\n",
    "Enable inverse-document-frequency reweighting.\n",
    "\n",
    "smooth_idf : boolean (default=True)\n",
    "Smooth idf weights by adding one to document frequencies, as if an extra document was seen containing every term in the collection exactly once. Prevents zero divisions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(78612, 98482)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Consider changing parameters per above\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Create feature matrix\n",
    "x = vectorizer.fit_transform(twitter_df[\"Text\"].tolist())\n",
    "print(x.shape)\n",
    "\n",
    "transformer = TfidfTransformer().fit(twitter_df[\"Text\"].tolist())\n",
    "tfidf_sample = tfidf_transformer.transform(bow_sample)\n",
    "print(tfidf_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into testing and training datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation?\n",
    "What cross-validation does is splitting the training data into a certain number of training folds \n",
    "(with 75% of the training data) and a the same number of testing folds (with 25% of the training data), \n",
    "use the training folds to train the classifier, and test it against the testing folds to obtain performance \n",
    "metrics (see below). The process is repeated multiple times and an average for each of the metrics is calculated.\n",
    "\n",
    "If your testing set is always the same, you might be overfitting to that testing set, which means you might \n",
    "be adjusting your analysis to a given set of data so much that you might fail to analyze a different set. \n",
    "Cross-validation helps prevent that.\n",
    "The more data you have, the more folds you will be able to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kept test size at default, which is .25\n",
    "just using part of the dataset now; use rest later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into testing and training datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(twitter_df[\"Text\"], twitter_df[\"Sentiment\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use gridsearch to do cross-validation\n",
    "Use complement MB b/c outperforms MNB (look at documentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONSIDER USING ADDITIONAL PARAMETERS FOR GRIDSEARCH AND ADDITIONAL FOLDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create transformation pipeline\n",
    "pipeline = Pipeline([\n",
    "    (\"vectorize\", TfidfVectorizer()),\n",
    "    (\"classify\", ComplementNB()),\n",
    "])\n",
    "\n",
    "# Set parameters for Gridsearch\n",
    "parameters = {\"vectorize__use_idf\": (True, False),\n",
    "              \"vectorize__ngram_range\": [(1,1), (1,2)],\n",
    "              \"vectorize__max_df\" : (1, .8),\n",
    "              \"vectorize__norm\": (\"l1\", \"l2\"),\n",
    "              \"classify__alpha\": (.8, 1),\n",
    "             }\n",
    "grid = GridSearchCV(pipeline, cv = 5, param_grid = parameters, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 160 out of 160 | elapsed: 11.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Model: 0.770756 using {'classify__alpha': 0.8, 'vectorize__max_df': 0.8, 'vectorize__ngram_range': (1, 2), 'vectorize__norm': 'l2', 'vectorize__use_idf': True}\n",
      "\n",
      "\n",
      "Mean: 0.480181 Stdev:(0.001880) with: {'classify__alpha': 0.8, 'vectorize__max_df': 1, 'vectorize__ngram_range': (1, 1), 'vectorize__norm': 'l1', 'vectorize__use_idf': True}\n",
      "Mean: 0.480181 Stdev:(0.001880) with: {'classify__alpha': 0.8, 'vectorize__max_df': 1, 'vectorize__ngram_range': (1, 1), 'vectorize__norm': 'l1', 'vectorize__use_idf': False}\n",
      "Mean: 0.480436 Stdev:(0.001584) with: {'classify__alpha': 0.8, 'vectorize__max_df': 1, 'vectorize__ngram_range': (1, 1), 'vectorize__norm': 'l2', 'vectorize__use_idf': True}\n",
      "Mean: 0.480436 Stdev:(0.001584) with: {'classify__alpha': 0.8, 'vectorize__max_df': 1, 'vectorize__ngram_range': (1, 1), 'vectorize__norm': 'l2', 'vectorize__use_idf': False}\n",
      "Mean: 0.532132 Stdev:(0.003343) with: {'classify__alpha': 0.8, 'vectorize__max_df': 1, 'vectorize__ngram_range': (1, 2), 'vectorize__norm': 'l1', 'vectorize__use_idf': True}\n",
      "Mean: 0.532132 Stdev:(0.003343) with: {'classify__alpha': 0.8, 'vectorize__max_df': 1, 'vectorize__ngram_range': (1, 2), 'vectorize__norm': 'l1', 'vectorize__use_idf': False}\n",
      "Mean: 0.531725 Stdev:(0.002866) with: {'classify__alpha': 0.8, 'vectorize__max_df': 1, 'vectorize__ngram_range': (1, 2), 'vectorize__norm': 'l2', 'vectorize__use_idf': True}\n",
      "Mean: 0.531725 Stdev:(0.002866) with: {'classify__alpha': 0.8, 'vectorize__max_df': 1, 'vectorize__ngram_range': (1, 2), 'vectorize__norm': 'l2', 'vectorize__use_idf': False}\n",
      "Mean: 0.753761 Stdev:(0.003967) with: {'classify__alpha': 0.8, 'vectorize__max_df': 0.8, 'vectorize__ngram_range': (1, 1), 'vectorize__norm': 'l1', 'vectorize__use_idf': True}\n",
      "Mean: 0.748486 Stdev:(0.002921) with: {'classify__alpha': 0.8, 'vectorize__max_df': 0.8, 'vectorize__ngram_range': (1, 1), 'vectorize__norm': 'l1', 'vectorize__use_idf': False}\n",
      "Mean: 0.746807 Stdev:(0.003414) with: {'classify__alpha': 0.8, 'vectorize__max_df': 0.8, 'vectorize__ngram_range': (1, 1), 'vectorize__norm': 'l2', 'vectorize__use_idf': True}\n",
      "Mean: 0.760986 Stdev:(0.003194) with: {'classify__alpha': 0.8, 'vectorize__max_df': 0.8, 'vectorize__ngram_range': (1, 1), 'vectorize__norm': 'l2', 'vectorize__use_idf': False}\n",
      "Mean: 0.761885 Stdev:(0.003226) with: {'classify__alpha': 0.8, 'vectorize__max_df': 0.8, 'vectorize__ngram_range': (1, 2), 'vectorize__norm': 'l1', 'vectorize__use_idf': True}\n",
      "Mean: 0.744009 Stdev:(0.004725) with: {'classify__alpha': 0.8, 'vectorize__max_df': 0.8, 'vectorize__ngram_range': (1, 2), 'vectorize__norm': 'l1', 'vectorize__use_idf': False}\n",
      "Mean: 0.770756 Stdev:(0.002275) with: {'classify__alpha': 0.8, 'vectorize__max_df': 0.8, 'vectorize__ngram_range': (1, 2), 'vectorize__norm': 'l2', 'vectorize__use_idf': True}\n",
      "Mean: 0.768483 Stdev:(0.001667) with: {'classify__alpha': 0.8, 'vectorize__max_df': 0.8, 'vectorize__ngram_range': (1, 2), 'vectorize__norm': 'l2', 'vectorize__use_idf': False}\n",
      "Mean: 0.480147 Stdev:(0.001913) with: {'classify__alpha': 1, 'vectorize__max_df': 1, 'vectorize__ngram_range': (1, 1), 'vectorize__norm': 'l1', 'vectorize__use_idf': True}\n",
      "Mean: 0.480147 Stdev:(0.001913) with: {'classify__alpha': 1, 'vectorize__max_df': 1, 'vectorize__ngram_range': (1, 1), 'vectorize__norm': 'l1', 'vectorize__use_idf': False}\n",
      "Mean: 0.480436 Stdev:(0.001584) with: {'classify__alpha': 1, 'vectorize__max_df': 1, 'vectorize__ngram_range': (1, 1), 'vectorize__norm': 'l2', 'vectorize__use_idf': True}\n",
      "Mean: 0.480436 Stdev:(0.001584) with: {'classify__alpha': 1, 'vectorize__max_df': 1, 'vectorize__ngram_range': (1, 1), 'vectorize__norm': 'l2', 'vectorize__use_idf': False}\n",
      "Mean: 0.532149 Stdev:(0.003340) with: {'classify__alpha': 1, 'vectorize__max_df': 1, 'vectorize__ngram_range': (1, 2), 'vectorize__norm': 'l1', 'vectorize__use_idf': True}\n",
      "Mean: 0.532149 Stdev:(0.003340) with: {'classify__alpha': 1, 'vectorize__max_df': 1, 'vectorize__ngram_range': (1, 2), 'vectorize__norm': 'l1', 'vectorize__use_idf': False}\n",
      "Mean: 0.531725 Stdev:(0.002884) with: {'classify__alpha': 1, 'vectorize__max_df': 1, 'vectorize__ngram_range': (1, 2), 'vectorize__norm': 'l2', 'vectorize__use_idf': True}\n",
      "Mean: 0.531725 Stdev:(0.002884) with: {'classify__alpha': 1, 'vectorize__max_df': 1, 'vectorize__ngram_range': (1, 2), 'vectorize__norm': 'l2', 'vectorize__use_idf': False}\n",
      "Mean: 0.754185 Stdev:(0.003608) with: {'classify__alpha': 1, 'vectorize__max_df': 0.8, 'vectorize__ngram_range': (1, 1), 'vectorize__norm': 'l1', 'vectorize__use_idf': True}\n",
      "Mean: 0.744229 Stdev:(0.003559) with: {'classify__alpha': 1, 'vectorize__max_df': 0.8, 'vectorize__ngram_range': (1, 1), 'vectorize__norm': 'l1', 'vectorize__use_idf': False}\n",
      "Mean: 0.749945 Stdev:(0.003725) with: {'classify__alpha': 1, 'vectorize__max_df': 0.8, 'vectorize__ngram_range': (1, 1), 'vectorize__norm': 'l2', 'vectorize__use_idf': True}\n",
      "Mean: 0.761936 Stdev:(0.004250) with: {'classify__alpha': 1, 'vectorize__max_df': 0.8, 'vectorize__ngram_range': (1, 1), 'vectorize__norm': 'l2', 'vectorize__use_idf': False}\n",
      "Mean: 0.759511 Stdev:(0.003433) with: {'classify__alpha': 1, 'vectorize__max_df': 0.8, 'vectorize__ngram_range': (1, 2), 'vectorize__norm': 'l1', 'vectorize__use_idf': True}\n",
      "Mean: 0.740226 Stdev:(0.004757) with: {'classify__alpha': 1, 'vectorize__max_df': 0.8, 'vectorize__ngram_range': (1, 2), 'vectorize__norm': 'l1', 'vectorize__use_idf': False}\n",
      "Mean: 0.770349 Stdev:(0.002212) with: {'classify__alpha': 1, 'vectorize__max_df': 0.8, 'vectorize__ngram_range': (1, 2), 'vectorize__norm': 'l2', 'vectorize__use_idf': True}\n",
      "Mean: 0.766041 Stdev:(0.001874) with: {'classify__alpha': 1, 'vectorize__max_df': 0.8, 'vectorize__ngram_range': (1, 2), 'vectorize__norm': 'l2', 'vectorize__use_idf': False}\n"
     ]
    }
   ],
   "source": [
    "grid.fit(X_train,y_train)\n",
    "# summarize results\n",
    "print(\"\\nBest Model: %f using %s\" % (grid.best_score_, grid.best_params_))\n",
    "print('\\n')\n",
    "means = grid.cv_results_['mean_test_score']\n",
    "stds = grid.cv_results_['std_test_score']\n",
    "params = grid.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"Mean: %f Stdev:(%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply algorithm (1 to begin)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
