{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "#from clean import clean\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from joblib import dump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 8836: expected 4 fields, saw 5\\n'\n",
      "b'Skipping line 535882: expected 4 fields, saw 7\\n'\n"
     ]
    }
   ],
   "source": [
    "twitter_df = pd.read_csv(\"Resources/sent_analysis_dataset.csv\", error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly select 10% tweets to train and test model\n",
    "twitter_df = twitter_df.sample(frac = 0.10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentSource</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>284211</th>\n",
       "      <td>284224</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>@littlescoop My son used to live near Scarboro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1434709</th>\n",
       "      <td>1434725</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>I am so sad... saying my first round of goodby...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1463657</th>\n",
       "      <td>1463673</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>in the office today, working hard.  Very nice ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262585</th>\n",
       "      <td>262598</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>@Laibcoms I see you but I don't see the site; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163789</th>\n",
       "      <td>163802</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>@DiscloseTV Encounters In Siberia 1/2 Access D...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ItemID  Sentiment SentimentSource  \\\n",
       "284211    284224          1    Sentiment140   \n",
       "1434709  1434725          0    Sentiment140   \n",
       "1463657  1463673          1    Sentiment140   \n",
       "262585    262598          0    Sentiment140   \n",
       "163789    163802          0    Sentiment140   \n",
       "\n",
       "                                             SentimentText  \n",
       "284211   @littlescoop My son used to live near Scarboro...  \n",
       "1434709  I am so sad... saying my first round of goodby...  \n",
       "1463657  in the office today, working hard.  Very nice ...  \n",
       "262585   @Laibcoms I see you but I don't see the site; ...  \n",
       "163789   @DiscloseTV Encounters In Siberia 1/2 Access D...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop and rename columns\n",
    "twitter_df = twitter_df.drop([\"ItemID\", \"SentimentSource\"], axis = 1)\n",
    "twitter_df = twitter_df.rename(columns = {\"SentimentText\": \"Text\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment    0\n",
       "Text         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check columns for missing data\n",
    "twitter_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment     int64\n",
       "Text         object\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify data are of correct type\n",
    "twitter_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify sentiment column has appropriate data\n",
    "twitter_df[\"Sentiment\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean data using clean function\n",
    "\n",
    "# twitter_df[\"Sentiment\"] = twitter_df[\"Sentiment\"].map(lambda x: clean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET RID OF THIS ONCE WE HAVE CLEAN FUNCTION\n",
    "twitter_df[\"Text\"] = twitter_df[\"Text\"].map(lambda x: re.sub(r\"[!#$%&'\\\\()*+,-./:;<=>?@\\^_`{|}~]\", \"\", x))\n",
    "twitter_df[\"Text\"] = twitter_df[\"Text\"].map(lambda x: re.sub(\"\\[\", \" \", x))\n",
    "twitter_df[\"Text\"] = twitter_df[\"Text\"].map(lambda x: re.sub(\"\\]\", \" \", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>284211</th>\n",
       "      <td>1</td>\n",
       "      <td>littlescoop My son used to live near Scarborou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1434709</th>\n",
       "      <td>0</td>\n",
       "      <td>I am so sad saying my first round of goodbyes ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1463657</th>\n",
       "      <td>1</td>\n",
       "      <td>in the office today working hard  Very nice ri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262585</th>\n",
       "      <td>0</td>\n",
       "      <td>Laibcoms I see you but I dont see the site Are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163789</th>\n",
       "      <td>0</td>\n",
       "      <td>DiscloseTV Encounters In Siberia 12 Access Den...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1137397</th>\n",
       "      <td>1</td>\n",
       "      <td>PantsPartay YAY how fun would it be 2 have a b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502818</th>\n",
       "      <td>0</td>\n",
       "      <td>My studio computer is doing some very crazy th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195675</th>\n",
       "      <td>0</td>\n",
       "      <td>icontips whats wrong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>784321</th>\n",
       "      <td>0</td>\n",
       "      <td>how lucky are those people</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162073</th>\n",
       "      <td>0</td>\n",
       "      <td>folieajade AWE JADE  Im here if you need to vent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1116086</th>\n",
       "      <td>1</td>\n",
       "      <td>NickRac Oops not the  you mean the post about ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1449614</th>\n",
       "      <td>1</td>\n",
       "      <td>I saw the Tonight Show with Jay Leno in person...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1360038</th>\n",
       "      <td>0</td>\n",
       "      <td>yequita  hes glued to me now</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1117294</th>\n",
       "      <td>0</td>\n",
       "      <td>Off to buy medicine for my soar throat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876310</th>\n",
       "      <td>1</td>\n",
       "      <td>if I dont hear your songs I would be a crazier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1086664</th>\n",
       "      <td>1</td>\n",
       "      <td>mrmwrites Ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749825</th>\n",
       "      <td>1</td>\n",
       "      <td>DonnieWahlberg Correction youre in HOTTLANTA g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1323118</th>\n",
       "      <td>1</td>\n",
       "      <td>watching gossip girllllllllllllllllllllllllllll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>774923</th>\n",
       "      <td>1</td>\n",
       "      <td>Hitting the yellow pages looking for YOU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109527</th>\n",
       "      <td>1</td>\n",
       "      <td>Bodzy85 Ah Lakeview Terrace Suprised that I ac...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Sentiment                                               Text\n",
       "284211           1  littlescoop My son used to live near Scarborou...\n",
       "1434709          0  I am so sad saying my first round of goodbyes ...\n",
       "1463657          1  in the office today working hard  Very nice ri...\n",
       "262585           0  Laibcoms I see you but I dont see the site Are...\n",
       "163789           0  DiscloseTV Encounters In Siberia 12 Access Den...\n",
       "1137397          1  PantsPartay YAY how fun would it be 2 have a b...\n",
       "1502818          0  My studio computer is doing some very crazy th...\n",
       "195675           0                              icontips whats wrong \n",
       "784321           0                        how lucky are those people \n",
       "162073           0   folieajade AWE JADE  Im here if you need to vent\n",
       "1116086          1  NickRac Oops not the  you mean the post about ...\n",
       "1449614          1  I saw the Tonight Show with Jay Leno in person...\n",
       "1360038          0                       yequita  hes glued to me now\n",
       "1117294          0            Off to buy medicine for my soar throat \n",
       "876310           1    if I dont hear your songs I would be a crazier \n",
       "1086664          1                                      mrmwrites Ok \n",
       "749825           1  DonnieWahlberg Correction youre in HOTTLANTA g...\n",
       "1323118          1   watching gossip girllllllllllllllllllllllllllll \n",
       "774923           1         Hitting the yellow pages looking for YOU  \n",
       "109527           1  Bodzy85 Ah Lakeview Terrace Suprised that I ac..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PARAMETERS IN TFIDVECTORIZER\n",
    "\n",
    "decode_error : {‘strict’, ‘ignore’, ‘replace’} (default=’strict’)\n",
    "Instruction on what to do if a byte sequence is given to analyze that contains characters not of the given encoding. By default, it is ‘strict’, meaning that a UnicodeDecodeError will be raised. Other values are ‘ignore’ and ‘replace’.\n",
    "\n",
    "IF TOO MANY FEATURES, ADJUST HERE\n",
    "max_df : float in range [0.0, 1.0] or int (default=1.0)\n",
    "When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words). If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "min_df : float in range [0.0, 1.0] or int (default=1)\n",
    "When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "DON'T CHANGE, BUT INCLUDE IN README THAT WE ARE DOING THE DEFAULTS AND WHY\n",
    "\n",
    "norm : ‘l1’, ‘l2’ or None, optional (default=’l2’)\n",
    "Each output row will have unit norm, either: * ‘l2’: Sum of squares of vector elements is 1. The cosine similarity between two vectors is their dot product when l2 norm has been applied. * ‘l1’: Sum of absolute values of vector elements is 1. See preprocessing.normalize\n",
    "\n",
    "use_idf : boolean (default=True)\n",
    "Enable inverse-document-frequency reweighting.\n",
    "\n",
    "smooth_idf : boolean (default=True)\n",
    "Smooth idf weights by adding one to document frequencies, as if an extra document was seen containing every term in the collection exactly once. Prevents zero divisions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into testing and training datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation?\n",
    "What cross-validation does is splitting the training data into a certain number of training folds \n",
    "(with 75% of the training data) and a the same number of testing folds (with 25% of the training data), \n",
    "use the training folds to train the classifier, and test it against the testing folds to obtain performance \n",
    "metrics (see below). The process is repeated multiple times and an average for each of the metrics is calculated.\n",
    "\n",
    "If your testing set is always the same, you might be overfitting to that testing set, which means you might \n",
    "be adjusting your analysis to a given set of data so much that you might fail to analyze a different set. \n",
    "Cross-validation helps prevent that.\n",
    "The more data you have, the more folds you will be able to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kept test size at default, which is .25\n",
    "just using part of the dataset now; use rest later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into testing and training datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(twitter_df[\"Text\"], twitter_df[\"Sentiment\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use gridsearch to do cross-validation\n",
    "Use complement MB b/c outperforms MNB (look at documentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create transformation pipeline\n",
    "pipeline = Pipeline([\n",
    "    (\"vectorize\", TfidfVectorizer()),\n",
    "    (\"classify\", ComplementNB()),\n",
    "])\n",
    "\n",
    "# Set parameters for Gridsearch\n",
    "parameters = {\"vectorize__use_idf\": (True, False),\n",
    "              \"vectorize__ngram_range\": [(1,1), (1,2)],\n",
    "              \"vectorize__max_df\" : (1, .8),\n",
    "              \"vectorize__norm\": (\"l1\", \"l2\"),\n",
    "              \"classify__alpha\": (.8, 1)\n",
    "             }\n",
    "nb = GridSearchCV(pipeline, param_grid = parameters, n_jobs = -1, cv = 5, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 160 out of 160 | elapsed:  7.9min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vectorize', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=T...abulary=None)), ('classify', ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False))]),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'vectorize__use_idf': (True, False), 'vectorize__ngram_range': [(1, 1), (1, 2)], 'vectorize__max_df': (1, 0.8), 'vectorize__norm': ('l1', 'l2'), 'classify__alpha': (0.8, 1)},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.7765530638962794 with parameters: {'classify__alpha': 0.8, 'vectorize__max_df': 0.8, 'vectorize__ngram_range': (1, 2), 'vectorize__norm': 'l2', 'vectorize__use_idf': False}\n"
     ]
    }
   ],
   "source": [
    "# Get best results\n",
    "print(f\"Best Score: {nb.best_score_} with parameters: {nb.best_params_}\")\n",
    "optimized_nb = nb.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.7796331019105053\n",
      "Confusion matrix: [[17096  2632]\n",
      " [ 6065 13673]]\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.87      0.80     19728\n",
      "           1       0.84      0.69      0.76     19738\n",
      "\n",
      "   micro avg       0.78      0.78      0.78     39466\n",
      "   macro avg       0.79      0.78      0.78     39466\n",
      "weighted avg       0.79      0.78      0.78     39466\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model using test data\n",
    "predictions = optimized_nb.predict(X_test)\n",
    "print(f\"Accuracy score: {accuracy_score(y_test, predictions)}\")\n",
    "print(f\"Confusion matrix: {confusion_matrix(y_test, predictions)}\")\n",
    "print(f\"Classification report: \\n{classification_report(y_test, predictions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try another algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline = Pipeline([\n",
    "#     (\"vectorize\", TfidfVectorizer()),\n",
    "#     (\"classify\", LinearRegression()),\n",
    "# ])\n",
    "\n",
    "# # Set parameters for Gridsearch\n",
    "# parameters = {\"vectorize__use_idf\": (True, False),\n",
    "#               \"vectorize__ngram_range\": [(1,1), (1,2)],\n",
    "#               \"vectorize__max_df\" : (1, .8),\n",
    "#               \"vectorize__norm\": (\"l1\", \"l2\"),\n",
    "#               \"classify__\n",
    "#              }\n",
    "# nb = GridSearchCV(pipeline, param_grid = parameters, n_jobs = -1, cv = 5, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['twitter_model.joblib']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save best model\n",
    "dump(optimized_nb, 'twitter_model.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
