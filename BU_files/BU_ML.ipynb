{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import spacy\n",
    "from spacy.attrs import ORTH, LEMMA, NORM, TAG\n",
    "from clean import replace_emoticons, clean_text, clean_tweet\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from joblib import dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import spacy nlp library\n",
    "nlp = spacy.load('en_core_web_sm',parser=False, entity=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 8836: expected 4 fields, saw 5\\n'\n",
      "b'Skipping line 535882: expected 4 fields, saw 7\\n'\n"
     ]
    }
   ],
   "source": [
    "twitter_df = pd.read_csv(\"Resources/sent_analysis_dataset.csv\", error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly select 10% tweets to train and test model\n",
    "twitter_df = twitter_df.sample(frac = 0.001) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentSource</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1418817</th>\n",
       "      <td>1418833</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>Haha, one can't xD but i finished it none the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640552</th>\n",
       "      <td>640568</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>done eating lunch..</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482902</th>\n",
       "      <td>482915</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>@secretfanofu i just read  that sux cuz people...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>739488</th>\n",
       "      <td>739504</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>Have to miss the area meeting tonight due to t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875109</th>\n",
       "      <td>875125</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>I want to come home from work. Im sick. -_- My...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ItemID  Sentiment SentimentSource  \\\n",
       "1418817  1418833          1    Sentiment140   \n",
       "640552    640568          1    Sentiment140   \n",
       "482902    482915          0    Sentiment140   \n",
       "739488    739504          0    Sentiment140   \n",
       "875109    875125          0    Sentiment140   \n",
       "\n",
       "                                             SentimentText  \n",
       "1418817  Haha, one can't xD but i finished it none the ...  \n",
       "640552                                done eating lunch..   \n",
       "482902   @secretfanofu i just read  that sux cuz people...  \n",
       "739488   Have to miss the area meeting tonight due to t...  \n",
       "875109   I want to come home from work. Im sick. -_- My...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop and rename columns\n",
    "twitter_df = twitter_df.drop([\"ItemID\", \"SentimentSource\"], axis = 1)\n",
    "twitter_df = twitter_df.rename(columns = {\"SentimentText\": \"Text\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment    0\n",
       "Text         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check columns for missing data\n",
    "twitter_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment     int64\n",
       "Text         object\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify data are of correct type\n",
    "twitter_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify sentiment column has appropriate data\n",
    "twitter_df[\"Sentiment\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text of tweets using previously defined clean_tweet function\n",
    " \n",
    "twitter_df[\"Text\"] = twitter_df[\"Text\"].map(lambda x: clean_tweet(x, nlp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PARAMETERS IN TFIDVECTORIZER\n",
    "\n",
    "decode_error : {‘strict’, ‘ignore’, ‘replace’} (default=’strict’)\n",
    "Instruction on what to do if a byte sequence is given to analyze that contains characters not of the given encoding. By default, it is ‘strict’, meaning that a UnicodeDecodeError will be raised. Other values are ‘ignore’ and ‘replace’.\n",
    "\n",
    "IF TOO MANY FEATURES, ADJUST HERE\n",
    "max_df : float in range [0.0, 1.0] or int (default=1.0)\n",
    "When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words). If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "min_df : float in range [0.0, 1.0] or int (default=1)\n",
    "When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "DON'T CHANGE, BUT INCLUDE IN README THAT WE ARE DOING THE DEFAULTS AND WHY\n",
    "\n",
    "norm : ‘l1’, ‘l2’ or None, optional (default=’l2’)\n",
    "Each output row will have unit norm, either: * ‘l2’: Sum of squares of vector elements is 1. The cosine similarity between two vectors is their dot product when l2 norm has been applied. * ‘l1’: Sum of absolute values of vector elements is 1. See preprocessing.normalize\n",
    "\n",
    "use_idf : boolean (default=True)\n",
    "Enable inverse-document-frequency reweighting.\n",
    "\n",
    "smooth_idf : boolean (default=True)\n",
    "Smooth idf weights by adding one to document frequencies, as if an extra document was seen containing every term in the collection exactly once. Prevents zero divisions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into testing and training datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation?\n",
    "What cross-validation does is splitting the training data into a certain number of training folds \n",
    "(with 75% of the training data) and a the same number of testing folds (with 25% of the training data), \n",
    "use the training folds to train the classifier, and test it against the testing folds to obtain performance \n",
    "metrics (see below). The process is repeated multiple times and an average for each of the metrics is calculated.\n",
    "\n",
    "If your testing set is always the same, you might be overfitting to that testing set, which means you might \n",
    "be adjusting your analysis to a given set of data so much that you might fail to analyze a different set. \n",
    "Cross-validation helps prevent that.\n",
    "The more data you have, the more folds you will be able to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kept test size at default, which is .25\n",
    "just using part of the dataset now; use rest later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into testing and training datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(twitter_df[\"Text\"], twitter_df[\"Sentiment\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use gridsearch to do cross-validation\n",
    "Use complement MB b/c outperforms MNB (look at documentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create transformation pipeline\n",
    "pipeline = Pipeline([\n",
    "    (\"vectorize\", TfidfVectorizer()),\n",
    "    (\"classify\", ComplementNB()),\n",
    "])\n",
    "\n",
    "# Set parameters for Gridsearch\n",
    "parameters = {\"vectorize__use_idf\": (True, False),\n",
    "              \"vectorize__ngram_range\": [(1,1), (1,2)],\n",
    "              \"vectorize__max_df\" : (1, .8),\n",
    "              \"vectorize__norm\": (\"l1\", \"l2\"),\n",
    "              \"classify__alpha\": (.8, 1)\n",
    "             }\n",
    "nb = GridSearchCV(pipeline, param_grid = parameters, n_jobs = -1, cv = 5, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  48 tasks      | elapsed:    4.0s\n",
      "[Parallel(n_jobs=-1)]: Done 160 out of 160 | elapsed:    7.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vectorize', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=T...abulary=None)), ('classify', ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False))]),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'vectorize__use_idf': (True, False), 'vectorize__ngram_range': [(1, 1), (1, 2)], 'vectorize__max_df': (1, 0.8), 'vectorize__norm': ('l1', 'l2'), 'classify__alpha': (0.8, 1)},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.6875 with parameters: {'classify__alpha': 0.8, 'vectorize__max_df': 0.8, 'vectorize__ngram_range': (1, 2), 'vectorize__norm': 'l2', 'vectorize__use_idf': False}\n"
     ]
    }
   ],
   "source": [
    "# Get best results\n",
    "print(f\"Best Score: {nb.best_score_} with parameters: {nb.best_params_}\")\n",
    "optimized_nb = nb.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.6987341772151898\n",
      "Confusion matrix: [[169  46]\n",
      " [ 73 107]]\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.79      0.74       215\n",
      "           1       0.70      0.59      0.64       180\n",
      "\n",
      "   micro avg       0.70      0.70      0.70       395\n",
      "   macro avg       0.70      0.69      0.69       395\n",
      "weighted avg       0.70      0.70      0.70       395\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model using test data\n",
    "predictions = optimized_nb.predict(X_test)\n",
    "print(f\"Accuracy score: {accuracy_score(y_test, predictions)}\")\n",
    "print(f\"Confusion matrix: {confusion_matrix(y_test, predictions)}\")\n",
    "print(f\"Classification report: \\n{classification_report(y_test, predictions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try another algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline = Pipeline([\n",
    "#     (\"vectorize\", TfidfVectorizer()),\n",
    "#     (\"classify\", LinearRegression()),\n",
    "# ])\n",
    "\n",
    "# # Set parameters for Gridsearch\n",
    "# parameters = {\"vectorize__use_idf\": (True, False),\n",
    "#               \"vectorize__ngram_range\": [(1,1), (1,2)],\n",
    "#               \"vectorize__max_df\" : (1, .8),\n",
    "#               \"vectorize__norm\": (\"l1\", \"l2\"),\n",
    "#               \"classify__\n",
    "#              }\n",
    "# nb = GridSearchCV(pipeline, param_grid = parameters, n_jobs = -1, cv = 5, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['twitter_model.joblib']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save best model\n",
    "dump(optimized_nb, 'twitter_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
